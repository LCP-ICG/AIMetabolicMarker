#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xrH1jDUOXuSX5hMribJ2kn3tPQQaHjD9
"""

import numpy as np
import pandas as pd
import scipy

import torch
from torch.utils.data import DataLoader
import torch.nn as nn

import matplotlib.pyplot as plt
import scipy.stats as stats
from statsmodels.stats.multitest import fdrcorrection as fdr

dir = ''
classes = pd.read_csv(dir + 'classes.csv', index_col= 0).to_dict()['Group']
df = pd.read_csv(dir + 'init.csv', index_col = 0)
final_losses = []

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.set_printoptions(precision = 6)
print(device)
torch.manual_seed(3047)
n_epochs = 20
b_size = 8
learning_rate = 0.001
multiplier = 500
arr_length = len(df.columns)


bottleneck_size_list = [i for i in range(2, arr_length + 1)]
#bottleneck_size_list = [100]

noise_factor = 0.25
input_size = arr_length


def getMetStdTensor(df):
    met_std = [] 
    for i in df:
        met_std.append(df[i].describe()[2])
    return torch.tensor(met_std)


class DAE(nn.Module):
    def __init__(self, input_size, bottleneck_size):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, bottleneck_size),
            nn.ReLU()
            )

        self.decoder = nn.Sequential(
            nn.Linear(bottleneck_size, input_size),
            nn.ReLU()
              )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

def addNoise(batch, noise_factor):
    
    noise = torch.normal(mean= torch.zeros(len(met_stds)), std= met_stds)
    noisy = batch + (noise * noise_factor).to(device)
    return noisy

def getControlDelDF(df, classes):
    control_ids = []
    del_ids = []
    for i in classes:
        if classes.get(i) == 'Control':
            control_ids.append(i)
        else:
            del_ids.append(i)
    df_control = df.loc[control_ids].copy().to_numpy()
    df_del = df.loc[del_ids].copy().to_numpy()

    return df_control, df_del

def plotLosses(train_loss, eval_loss, i):
    y = train_loss
    x = range(len(train_loss))
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))
    fig.suptitle(str(i) + ' bottleneck_size' + '\n' + 'train err = ' + str(train_loss[-1]) + ' eval err = ' +str(eval_loss[-1]) , fontsize=15)
    ax1.plot(x, y)
    ax1.set_title('train')
    for index in range(len(x)):
      ax1.text(x[index], y[index], y[index], size=6)
    y1 = eval_loss
    x1 = range(len(eval_loss))
    ax2.plot(x1, y1)
    ax2.set_title('eval')
    for index in range(len(x1)):
      ax2.text(x1[index], y1[index], y1[index], size=6)
    plt.savefig(str(i) + '.png')

def train(data, data_del):
    multiplied_data = torch.repeat_interleave(data, multiplier, 0)
    split_num = len(multiplied_data) // 5
    train_d, test_d = torch.utils.data.random_split(multiplied_data, [len(multiplied_data) - split_num, split_num])
    train_data = DataLoader(train_d, batch_size = b_size, shuffle = True)
    test_data = DataLoader(test_d, batch_size = b_size, shuffle = True)
    outputs_con = {}
    outputs_del = {}
    for i in bottleneck_size_list:
            ae = DAE(input_size, i).to(device)
            opt = torch.optim.Adam(ae.parameters(), lr = learning_rate)
            criterion = torch.nn.MSELoss()
            train_loss = []
            eval_loss = []
            for epoch in range(n_epochs):
              ae.train()
              train_loss_epoch = []
              for batch in train_data:
                original = batch.to(device)
                noisy = addNoise(batch, noise_factor).to(device)
                output_e, output_d = ae(noisy)
                loss_value = criterion(output_d, original)
                opt.zero_grad()
                loss_value.backward()
                opt.step()
                train_loss_epoch.append(loss_value.detach().cpu().numpy())
              train_loss.append(np.mean(train_loss_epoch))
              ae.eval()
              test_loss_epoch = []
              with torch.no_grad():
                for batch in test_data:
                    original = batch.to(device)
                    noisy = addNoise(batch, noise_factor).to(device)
                    output_e, output_d = ae(noisy)
                    loss_eval_value = criterion(output_d, original)
                    test_loss_epoch.append(loss_eval_value.detach().cpu().numpy())
              eval_loss.append(np.mean(test_loss_epoch))
            print('Training NN with ' + str(noise_factor) + ' noise factor and ' + str(i) + ' bottleneck size is over')
            #plotLosses(train_loss, eval_loss, i)
            final_losses.append([train_loss[-1], eval_loss[-1], i])
            ae.eval()
            con_output = []
            for k in data:
                with torch.no_grad():
                    encoded, decoded = ae(k)
                    con_output.append(decoded.cpu().numpy())
            ae.eval()
            del_output = []
            for k in data_del:
              with torch.no_grad():
                encoded, decoded = ae(k)
                del_output.append(decoded.cpu().numpy())
            torch.save(ae.state_dict(), 'model_' + str(i) + '.pt')
            torch.save(torch.tensor(np.array(con_output)), 'con_tensor' + str(i) + '.pt')
            torch.save(torch.tensor(np.array(del_output)), 'del_tensor' + str(i) + '.pt')
            outputs_con[i] = con_output
            outputs_del[i] = del_output
    return outputs_con, outputs_del

def getPValue(inputs_con, inputs_del, outputs_con, outputs_del):
    mw_diff_abs = []
    mw_diff = []
    for i in range(arr_length):
        dels_diff = outputs_del[:, i] - inputs_del[:, i]
        sqr_dels_diff = np.square(dels_diff)
        con_diff = outputs_con[:, i] - inputs_con[:, i]
        sqr_con_diff = np.square(con_diff)
        #mw_diff.append(stats.mannwhitneyu(sqr_dels_diff, sqr_con_diff).pvalue)
        mw_diff.append(stats.ks_2samp(sqr_dels_diff, sqr_con_diff, alternative='two-sided', method='auto').pvalue)
    fdr_diff = fdr(mw_diff)[1]
    return mw_diff, fdr_diff

if __name__ == "__main__":
    met_stds = getMetStdTensor(df)
    df_control, df_del = getControlDelDF(df, classes)
    init_tensor_control = torch.tensor(df_control).to(torch.float32).to(device)
    init_tensor_del = torch.tensor(df_del).to(torch.float32).to(device)
    outputs_con, outputs_del = train(init_tensor_control, init_tensor_del)
    pv = {}
    for i in outputs_con:
        pv[i] = getPValue(df_control, df_del, np.array(outputs_con[i]), np.array(outputs_del[i]))
    columns = []
    rows = []
    for i in pv:
        columns.append(i)
    for i in df.columns:
        rows.append(i)
    fdrs = []
    mws = []
    for i in pv:
        fdrs.append(pv[i][1])
        mws.append(pv[i][0])
    mws_tr = [list(i) for i in zip(*mws)]
    fdrs_tr = [list(i) for i in zip(*fdrs)]
    mws_df = pd.DataFrame(mws_tr, index = rows, columns = columns)
    fdrs_df = pd.DataFrame(fdrs_tr, index = rows, columns = columns)
    mws_df.to_csv('mws.csv')
    fdrs_df.to_csv('fdrs.csv')
    counts = (fdrs_df < 0.05).sum(axis=1).sort_values(ascending=False)
    counts.to_csv('results.csv')
    pd.DataFrame(np.array(final_losses)).to_csv('losses.csv')

